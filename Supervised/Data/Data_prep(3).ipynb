{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_prep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdPaL2XtQt94",
        "outputId": "e14dab2c-368e-4f7f-990f-4c7825ae3444"
      },
      "source": [
        "#import os to provide access to functionality dependent on Operating system, The sys module is tells about which Python script is interacting with the host system\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Define the path to download  the packages\n",
        "nb_path = '/content/notebooks'\n",
        "\n",
        "#Link the path where our notebooks is saved\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLWwAA1IQzI1",
        "outputId": "5673d09d-30dd-48e1-827a-7ae417c4ccc4"
      },
      "source": [
        "!pip install --target=$nb_path textsearch "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 3.0MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 8.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85387 sha256=11cd9d4451d87f38effc38def647ab8b2c758cf10c26dccc0e659b4133e7268d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch\n",
            "Successfully installed anyascii-0.2.0 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/anyascii-0.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/ahocorasick.cpython-37m-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/textsearch already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/pyahocorasick-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/anyascii already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/textsearch-0.0.21.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjrhICd5RA5u",
        "outputId": "981f5d73-fe62-4c62-9125-fab298ac19ce"
      },
      "source": [
        "!pip install --target=$nb_path contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Using cached https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842/pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl\n",
            "Collecting anyascii\n",
            "  Using cached https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/anyascii-0.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/data already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/ahocorasick.cpython-37m-x86_64-linux-gnu.so already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/textsearch already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/contractions-0.0.48.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/pyahocorasick-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/anyascii already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/textsearch-0.0.21.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/contractions already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzsKWN3gRHEb",
        "outputId": "afec8488-9fc5-4f60-dff8-d14c143ae1ac"
      },
      "source": [
        "!pip install --target=$nb_path tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 51kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 61kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "Successfully installed tqdm-4.60.0\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/tqdm already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/tqdm-4.60.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROmoKelyir2T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CW5qfGOlcgt",
        "outputId": "48f65074-a7a8-4816-be33-5cde9a926682"
      },
      "source": [
        "!pip install --target=$nb_path num2words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=c0dc2a4acaec0bc24a2d82da030cfa28c968d52ad69ce90f77e3e7449045ee9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.10\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/num2words already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/docopt.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/docopt-0.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/num2words-0.5.10.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
            "\u001b[33mWARNING: Target directory /content/notebooks/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw1jAQYo59RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb90c5b5-5152-4290-a348-98cef95e97bc"
      },
      "source": [
        "# import nltk, csv, re packages\n",
        "import nltk\n",
        "import csv\n",
        "import re\n",
        "\n",
        "#Download the punkt package to remove punctuations\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Download the stopwors package to remove common stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee6y6nS6RDBV"
      },
      "source": [
        "import pandas as pd                        #Import pandas package as pd\n",
        "import contractions                        #Import contractions package\n",
        "from num2words import num2words            #Import num2words package\n",
        "from nltk.tokenize import word_tokenize    #Import the word_tokenizer package\n",
        "from nltk.tokenize import regexp_tokenize  #Import the regexp_tokenizer package\n",
        "from nltk.stem import WordNetLemmatizer    #Import the WordNetLemmatizer package\n",
        "from nltk.stem import PorterStemmer        #Impport the Porter stemmer package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aWWd106lOvI"
      },
      "source": [
        "# Load the Chat messages with design parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz14thUKSQri"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Data/Engine_design/Designintent1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS0K4qCppEoX"
      },
      "source": [
        "df2 = pd.read_csv(\"/content/notebooks/Data/DesignIntent2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lOUOHiDzWUQ-",
        "outputId": "b288a32b-ba99-46fb-dd76-54950727b2ed"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>body</th>\n",
              "      <th>intent_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi piston! My crankshaft depends on your bore ...</td>\n",
              "      <td>NAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thank you!</td>\n",
              "      <td>NAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How low can you go on the piston bore diameter?</td>\n",
              "      <td>Exploration of design parameter values</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>all my fos values are in the hundreds, i suspe...</td>\n",
              "      <td>Effects of design parameters on objectives</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hi flywheel! My crankshaft depends on your fly...</td>\n",
              "      <td>Dependencies between design parameters</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                body                                    intent_2\n",
              "0  Hi piston! My crankshaft depends on your bore ...                                         NAN\n",
              "1                                         thank you!                                         NAN\n",
              "2    How low can you go on the piston bore diameter?      Exploration of design parameter values\n",
              "3  all my fos values are in the hundreds, i suspe...  Effects of design parameters on objectives\n",
              "4  Hi flywheel! My crankshaft depends on your fly...      Dependencies between design parameters"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmfomhFnmPqt"
      },
      "source": [
        "# Create the new dataframe with chat data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-cHoux_V3jT"
      },
      "source": [
        "df.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
        "df.drop(\"a\",axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKkS14jfylig"
      },
      "source": [
        "# Copy the chat conversations to new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch7QIMO0u667"
      },
      "source": [
        "df1 = df[['body']].astype('str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kphAo4LIRam"
      },
      "source": [
        "df.drop_duplicates(keep=False, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shgCvGnew8sN",
        "outputId": "b4ce1938-5c53-435f-d8a4-e6cb2355244b"
      },
      "source": [
        "df1.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 629 entries, 0 to 738\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   body    629 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 9.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sE_xqHstpTd"
      },
      "source": [
        "#  Step1: Expanding the contracted words using contractions library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW-QWwZAgZ5D"
      },
      "source": [
        "df1['expan_contra']=\"\"\n",
        "for i in range(len(df1['body'])):\n",
        "   df1.loc[[i],['expan_contra']] = contractions.fix(df1['body'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNAzgVTfTgub"
      },
      "source": [
        "# Step2: Expanding the Abbrevations by comparing the words with slang file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MZTHI-dTlpJ"
      },
      "source": [
        "def expabr(user_string):\n",
        "    user_string = user_string.split(\" \")\n",
        "    # File path which consists of Abbreviations.\n",
        "    fileName = \"/content/drive/MyDrive/Colab Notebooks/Data/slang.txt\"\n",
        "    j = 0\n",
        "\n",
        "    for _str in user_string:\n",
        "      # File Access mode [Read Mode]\n",
        "      accessMode = \"r\"\n",
        "      with open(fileName, accessMode) as myCSVfile:\n",
        "      # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
        "          dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "          for row in dataFromFile:\n",
        "# Check if selected word matches short forms[LHS] in text file.\n",
        "            if _str.upper() == row[0]:  \n",
        "              user_string[j] = row[1]   \n",
        "          myCSVfile.close()\n",
        "      j = j + 1\n",
        "    # Replacing commas with spaces for final output.\n",
        "    return ' '.join(user_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEv74Pg1UBkb"
      },
      "source": [
        "df1['body_abbr']=\"\"\n",
        "for i in range(len(df1['body'])): \n",
        "  df1['body_abbr'][i] = expabr(df1['expan_contra'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYxzdxUQH-T"
      },
      "source": [
        "# Expanding the symbols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOciqv8zsDV7"
      },
      "source": [
        "def expsym(user_string):\n",
        "    user_string = re.sub(\";\", \" ; \", user_string)\n",
        "    user_string = re.sub(\"\\\\?\", \" ? \", user_string)\n",
        "    user_string = re.sub(\"=\", \" equals to \", user_string)\n",
        "    user_string = re.sub(\":\", \" : \", user_string)\n",
        "    user_string = re.sub(\">\", \" greater than \", user_string)\n",
        "    user_string = re.sub(\"<\", \" lesser than \", user_string)\n",
        "    user_string = re.sub(\"_\", \" _ \", user_string)\n",
        "    user_string = re.sub(\"[^0-9a-zA-Z=\\s:;?_-]+\", \" \",user_string)\n",
        "    user_string = ' '.join(re.split('(\\d+)', user_string))\n",
        "    user_string = user_string.split(\" \")\n",
        "    # File path which consists of Abbreviations.\n",
        "    fileName = \"/content/drive/MyDrive/Colab Notebooks/Data/symbols.txt\"\n",
        "    j = 0\n",
        "\n",
        "    for _str in user_string:\n",
        "      # File Access mode [Read Mode]\n",
        "      accessMode = \"r\"\n",
        "      with open(fileName, accessMode) as myCSVfile:\n",
        "      # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
        "          dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
        "          for row in dataFromFile:\n",
        "# Check if selected word matches short forms[LHS] in text file.\n",
        "            if _str == row[0]:  \n",
        "              user_string[j] = row[1]   \n",
        "          myCSVfile.close()\n",
        "      j = j + 1\n",
        "    # Replacing commas with spaces for final output.\n",
        "    return ' '.join(user_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs5bMcNWeoVs"
      },
      "source": [
        "df1['body_syexp']=\"\"\n",
        "for i in range(len(df1['body'])):\n",
        "  df1['body_syexp'][i] = expsym(df1['body_abbr'][i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5z5XNVcNq0Q"
      },
      "source": [
        "# Step4: Tokenising the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Su0LIyNp_j"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHeRa_QFOZPc"
      },
      "source": [
        "def senttok(user_string):\n",
        "  tokenized_text=sent_tokenize(user_string)\n",
        "  return tokenized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XXeOKNTSOTX"
      },
      "source": [
        "df1['sen_token'] = \"\"\n",
        "for i in range(len(df1['body'])):\n",
        "  df1['sen_token'][i] = senttok(df1['body_syexp'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-rGUxRjTSbO"
      },
      "source": [
        "# Step3 : Tokenising the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQwEym2cTPM1"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjX_U2-sGq4_"
      },
      "source": [
        "def wordtok(user_string):\n",
        "  tokenized_word=word_tokenize(user_string)\n",
        "  return tokenized_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ShOkyrIGzpd"
      },
      "source": [
        "df1['word_token']=\"\"\n",
        "for i in range(len(df1['body'])):\n",
        " df1['word_token'][i] = wordtok(df1['body_syexp'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKCqcnRXpOxh"
      },
      "source": [
        "# Creating single text corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFGr0ioEjjZl"
      },
      "source": [
        "df2 = df1[['body_syexp']]\n",
        "ndf = df2.unstack().to_frame().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLAjKpb-l2Gw"
      },
      "source": [
        "ndf['Design conversation dataset'] = ndf[ndf.columns[0:]].apply(\n",
        "    lambda x: ''.join(x.dropna().astype(str)),\n",
        "    axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SotPxzOfl-yN"
      },
      "source": [
        "ndf['Design conversation dataset'].to_csv('/content/drive/MyDrive/Colab Notebooks/Data/designcorpus_data', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2A5IlQiqc7U"
      },
      "source": [
        "# Step4: Remove the stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_pO0P5Zqjal"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXPRc1hRL9gZ",
        "outputId": "7868733c-1425-4d38-b9a8-02ca2a8b8336"
      },
      "source": [
        "df1[\"no_stpwrd\"] = \"\"\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in text if word not in stop_words])\n",
        "\n",
        "\n",
        "df1[\"no_stpwrd\"] = df1[\"word_token\"].apply(lambda text: remove_stopwords(text))\n",
        "df1[\"no_stpwrd\"].head(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     Hi piston My crankshaft depends bore diameter ...\n",
              "1                                                 thank\n",
              "2                     How low go piston bore diameter ?\n",
              "3     factorofsafety values hundreds suspect depends...\n",
              "4     Hi flywheel My crankshaft depends flywheel sha...\n",
              "5                                                min 40\n",
              "6                    What value works best simulation ?\n",
              "7                              Do value bore diameter ?\n",
              "8                               let play around numbers\n",
              "9     My flywheel also depends crankshaft bearing of...\n",
              "10    I broadcasted recent simulation definitely fin...\n",
              "11                                          Sounds good\n",
              "12                                        yes broadcast\n",
              "13          Sounds good I tried middle values broadcast\n",
              "14                                    Thank information\n",
              "15    Hey I conrod I curious thought good piston dia...\n",
              "16                                      45 seems decent\n",
              "17    How flywheel thickness factor bearing support ...\n",
              "18         Hi I need piston diameter pistonborediameter\n",
              "19    My design depends piston diameter happens decr...\n",
              "20    Increasing value bearingoffset presents two ad...\n",
              "21                            Alright I definitely work\n",
              "22               45 seems decent lower need reduce mass\n",
              "23    alright factorofsafety getting lowest factorof...\n",
              "24                                                    ?\n",
              "Name: no_stpwrd, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4nIfIXjq2jW"
      },
      "source": [
        "df1[\"stpwrd\"] = \"\"\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in text if word in stop_words])\n",
        "\n",
        "#check this\n",
        "df1[\"stpwrd\"] = df1[ \" \"].apply(lambda text: remove_stopwords(text))\n",
        "df1[\"stpwrd\"].head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwusDJ5-srmN"
      },
      "source": [
        "# Convert to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5y_4i-ospsl"
      },
      "source": [
        "# Create a new column as not to interfere with the existing data\n",
        "df1['lw_case_wordt']= \"\"\n",
        "\n",
        "def lower_casing(text):\n",
        "  return \" \".join(text).lower()\n",
        "\n",
        "df1['lw_case_wordt'] = df1[\"no_stpwrd\"].apply(lambda text: lower_casing(text))\n",
        "df1['lw_case_wordt'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv4wyAI9Mum3"
      },
      "source": [
        "# Create a new column as not to interfere with the existing data\n",
        "df1['lw_case']= \"\"\n",
        "\n",
        "def lower_casing(text):\n",
        "  return text.lower()\n",
        "\n",
        "df1['lw_case'] = df1['no_stpwrd'].apply(lambda text: lower_casing(text))\n",
        "df1['lw_case'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik9q_zpiB0Ii"
      },
      "source": [
        "# Stemming the Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pIa_r9L_AJ7"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxT_rY5wEGcP"
      },
      "source": [
        "def stem_words(text):\n",
        "  \"\"\"function to stemm the words\"\"\"\n",
        "  return ps.stem(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YM7WOkgUfZN"
      },
      "source": [
        "df1['stemm'] =df1[\"lw_case\"].apply(lambda text: stem_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bckCMMkgQw28"
      },
      "source": [
        "df1['stemm_stpword'] =df1[\"lw_case_wordt\"].apply(lambda text: stem_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL_QM5fAuj-R"
      },
      "source": [
        "# Import packages for Lemmatize the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4kS0Hs5wepr",
        "outputId": "be5841bc-b4f9-48a6-ea50-5f76c937f7d7"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFb-uKyGo_4j"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJsrplrKbQt"
      },
      "source": [
        "# Lemmatize body without stop words using Verb and Noun tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC0ZWLNQwwgD"
      },
      "source": [
        "def lemmatize_words(text):\n",
        "  \"\"\"custom function to lemmatise words\"\"\"\n",
        "  pos_tagged_text = nltk.pos_tag(wordtok(text))\n",
        "  return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.VERB)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df1[\"lemma\"] = df1[\"lw_case_wordt\"].apply(lambda text: lemmatize_words(text))\n",
        "df1[\"lemma\"] .head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1OzOxWXwbjf"
      },
      "source": [
        "def lemmatize_words(text):\n",
        "  \"\"\"custom function to lemmatise words\"\"\"\n",
        "  pos_tagged_text = nltk.pos_tag(wordtok(text))\n",
        "  return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df1[\"lemma\"] = df1[\"lemma\"].apply(lambda text: lemmatize_words(text))\n",
        "df1[\"lemma\"] .head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg1czn-aKwe_"
      },
      "source": [
        "# Lemmatize body with stop words using Verb and Noun tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTMowydaKt8x"
      },
      "source": [
        "def lemmatize_words(text):\n",
        "  \"\"\"custom function to lemmatise words\"\"\"\n",
        "  pos_tagged_text = nltk.pos_tag(wordtok(text))\n",
        "  return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.VERB)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df1[\"lemma_stpword\"] = df1[\"lw_case_wordt\"].apply(lambda text: lemmatize_words(text))\n",
        "df1[\"lemma_stpword\"] .head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO8x3RWtK34z"
      },
      "source": [
        "def lemmatize_words(text):\n",
        "  \"\"\"custom function to lemmatise words\"\"\"\n",
        "  pos_tagged_text = nltk.pos_tag(wordtok(text))\n",
        "  return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
        "\n",
        "df1[\"lemma_stpword\"] = df1[\"lemma_stpword\"].apply(lambda text: lemmatize_words(text))\n",
        "df1[\"lemma_stpword\"] .head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCtogCoq5Zng"
      },
      "source": [
        "intent_data = df1[[\"no_stpwrd\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBuKwj9o9k4v",
        "outputId": "842bd9d8-a405-45d1-8f59-531c11b8c3c9"
      },
      "source": [
        "intent_data[\"intent\"]=df2[[\"intent_2\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R73oliL7r3E",
        "outputId": "266c8e8b-8cae-468e-8ab6-67a259264999"
      },
      "source": [
        "intent_data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['no_stpwrd', 'intent'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04p6RXUto_FQ"
      },
      "source": [
        "intent_data.to_csv('/content/drive/MyDrive/Colab Notebooks/Data/Intentfile2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}