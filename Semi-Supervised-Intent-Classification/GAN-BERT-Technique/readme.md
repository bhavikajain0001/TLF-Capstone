**GAN-BERT Technique:**

Approach: The second method we employ under semi-supervised intent classification is the GAN-BERT technique. GAN-BERT combines the power of BERT and SS-GAN. The generator creates "fake" instances using a 100-dimensional noise vector generated from a Gaussian Distribution as an input. The discriminator is an MLP built on top of BERT that takes as input either a fake vector generated by the generator or a vector formed by BERT from real data. The discriminator's final layer is a softmax layer that produces a k+1 dimension vector of logits, where k is the number of classes in the dataset. The real data is split into two categories: labeled (L) data and unlabeled (U) data. The discriminator's goal is to determine whether or not the input is a real instance. If the input is predicted to be a real instance, it must also be predicted to which class the input belongs. The discriminator loss and generator loss are two competing losses that the training process strives to optimize. The discriminator loss is made up of two other losses: the supervised and unsupervised losses. Unsupervised loss measures the error in incorrectly recognizing a real (unlabeled) example as fake and not recognizing a fake example among the original k categories, whereas supervised loss measures the error in incorrectly recognizing a real (unlabeled) example as fake and not recognizing a fake example among the original k categories. The sum of two other losses, feature matching, and unsupervised loss, results in the generator loss. Unsupervised loss quantifies the error caused by fake instances successfully identified by the discriminator, whereas feature matching loss ensures that the generator produces instances whose intermediate representations presented in input to the discriminator are highly similar to the genuine ones.

During training, each class's samples are repeated by a factor of log(2|U|/|L|), ensuring the presence of some labeled instances in each batch and avoiding divergences due to the adversarial training's unsupervised component. The generator is removed from the design during the inference phase, while the remainder is kept.

![download](https://user-images.githubusercontent.com/26361255/120370102-c8867180-c331-11eb-9a6a-8148390f7a59.png)

Folder Structure:
The project contains two folders by the name of intent-1 and intent-2, containing files for the respective intents. 
In addition, the Data_Preparation.ipynb Jupyter notebook contains the data cleaning and required preprocessing. It outputs five files for each intent: train.tsv for training labeled data points, test.tsv contains the test utterances, valid.tsv contains the validation utterances, unlabeled.tsv for unlabeled data points, and test_OOS.tsv for out-of-space samples which the model has never seen to finally test the model. We have divided the labeled material into train, test, and validation split by 70%, 20%, and 10% and divided the unlabeled data into 60% unlabeled annotations and 40% test-out-of-space samples.

Inside the intent folders:
The TensorFlow code implementation of the original BERT model has been taken as a reference for this project.
Reference:https://github.com/google-research/bert

1. predict_ganbert_intent_classification_1.ipynb: This Jupyter file dynamically runs all the other python program files in a given folder <intent_1> by executing a bash script file. It calls run_experiment.sh bash file for training the model with a finalized set of hyperparameters, run_ganbert.sh bash file for label prediction on test and validation samples and finally, calculate inferences and performance metric by comparing the results on test and validation data points. Finally, the run_ganbert_OOS.sh bash file predicts final labels for out-of-space samples with the trained model and hence, we have final labels for all the instances in the dataset.
2. ganbert.py: Thi file contains the code for creating and defining generator, discriminator, classification model, and a function that runs evaluation tests on different files. It internally calls functions from other py files including modeling.py, modeling.py, tokenization.py, and data_processors.py.
3. data_preprocessors.py: This file does preprocess steps for the BERT model including generating a single training/test example for simple sequence classification by Unicode conversion and necessary splitting of text (for all the five files generated as a part of Data_Preparation notebook).
4. modeling.py: This file contains the necessary configuration of the BERT model.
5. optimization.py: This file contains the functions and classes related to optimization and fine-tuning (weight updates) for GAN-BERT.
6. tokenization.py: This file runs basic checks such as whether the casing configuration is consistent with the checkpoint name, converts a sequence of [tokens|ids] using the vocabulary, and finally runs end-to-end tokenization by putting BasicTokenizer and WordPieceTokenizer to use.
7. run_experiment.sh: With the help of this script file, we run all .py extension files which are located in the given folder path for label prediction on training samples.
8. run_ganbert.sh: With the help of this script file, we run all .py extension files which are located in the given folder path for label prediction on test and validation samples.
9. run_ganbert_OOS.sh: With the help of this script file, we run all .py extension files which are located in the given folder path for label prediction on out-of-space samples.
10. general_statistics_GANBERT0.05.txt: This is the output statistics file generated after running the run_experiment.sh bash file for training the model. It contains model performance metrics including accuracy, f1_macro, f1_micro, final loss, precision, recall, global step, etc.
11. input_folder: This folder contains the input files generated by Data_Preparation.ipynb (intent-wise)
12. ganbert_output_model: This folder is generated after running predict_ganbert_intent_classification_1.ipynb and contains all the output files with respect to training, test and validation and prediction on OOS samples.
