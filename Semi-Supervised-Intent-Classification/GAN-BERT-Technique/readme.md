GAN-BERT combines the power of BERT and SS-GAN. In this notebook, we are using it for intent classification. The generator produces "fake" examples by taking the input of a 100-dimensional noise vector drawn from Gaussian Distribution. The discriminator is an MLP on top of BERT which receives the input vector either a fake vector generated by the generator or the vector from the real data generated by BERT. The final layer of the discriminator is a softmax layer which outputs the k+1 dimension vector of logits, where k is the number of classes in the dataset. Here, the real data is divided into 2, they are labelled (L) and unlabelled (U) data.

The discriminator aims to classify whether the input is a real instance or not. If it predicts the input as a real instance, then it has to predict which class the input belongs to.

The training process tries to optimize two competing losses, they are discriminator loss and generator loss. The discriminator loss is the summation of 2 other losses: supervised and unsupervised loss. Supervised loss measures the error in assigning the wrong class to a real example among the original k categories, while unsupervised loss measures the error in incorrectly recognizing a real (unlabeled) example as fake and not recognizing a fake example. The generator loss is also the result of summation from 2 other losses: feature matching and unsupervised loss. Feature matching loss aims to make sure that the generator should produce examples whose intermediate representations provided in input to the discriminator are very similar to the real ones, while unsupervised loss measures the error induced by fake examples correctly identified by the discriminator.

During training, the samples in each class are replicated in the factor of log(2|U|/|L|), to guarantees the presence of some labelled instances in each batch to avoid divergences due to the unsupervised component of the adversarial training. During inference process, the generator is discarded from the architecture while retaining the rest.
