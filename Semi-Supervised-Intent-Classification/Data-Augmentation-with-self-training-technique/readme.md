Semi Supervised Intent classification using Self-Training Technique with different Data Augmentation methods:


The second method we employ under semi-supervised intent classification is using the self-training semi-supervised technique with data augmentation, to classify labels. To encode each sentence, the methodology uses modern transformer-based (XL-Net) models, and several data-augmentation approaches to parse the given sentences as enhanced data. We used data augmentations to standardize the label distributions and computed supervised loss throughout the training procedure for labeled phrases. We investigated self-training for unlabeled sentences by treating low-entropy predictions as pseudo labels and high-confidence predictions as labeled data for training. We also added consistency regularisation as an unsupervised loss following unlabeled data augmentations, based on the notion that the model should predict similar class distributions with original unlabeled sentences and supplemented phrases as input. With the help of a set of experiments, we demonstrated that our system performs extremely well in terms of F1-score and accuracy in predicting labels for the entire dataset.
